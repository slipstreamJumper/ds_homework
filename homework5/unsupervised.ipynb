{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Introduction\n", "\n", "### Grading (33pts total)\n", "* plot_images - 3pts\n", "* KMeans init - (ungraded)\n", "* KMeans remaining functions - 3pts, 3pts, 2pts\n", "* Entropy - 2pts\n", "* Total Entropy - 2pts\n", "* GM init params - 3pts\n", "* GM E step - 3pts\n", "* GM M step - 3pts\n", "* GM train - 2pts\n", "* predict_face - 7pts\n", "\n", "### Notebook Restrictions\n", "* You cannot use the sklearn package. You are free to use anything else. \n", "\n", "In this question, you will explore various unsupervised learning techniques in the context of facial recognition. We've prefetched and processed a dataset of faces from Faces in the Wild, which is a collection of recognized pictures of faces (http://vis-www.cs.umass.edu/lfw/). We have cropped, resized, and turned a subset of the images into black and white images for this assignment. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["The faces are stored in `faces_all.txt` as an array of integers indicating the grayscale on a 256 scale. The names of the person in each photo is in `labels_all.txt`. \n", "\n", "Debugging tips: To ensure your algorithm has the correct behavior, we suggest the following baseline test (since debugging on high dimensional data is significantly harder and more time consuming): \n", "1. Use `np.random.mulivariate_normal` to samples from 2+ multivariate gaussians in 2D space (for easy visualization)\n", "2. Ensure that your clustering algorithm is able to correctly cluster the gaussians you sampled from. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import scipy.sparse as sp\n", "import matplotlib\n", "matplotlib.use(\"svg\")\n", "# AUTOLAB_IGNORE_START\n", "%matplotlib inline\n", "# AUTOLAB_IGNORE_STOP\n", "import matplotlib.pyplot as plt\n", "plt.style.use(\"ggplot\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# AUTOLAB_IGNORE_START\n", "data = np.loadtxt(\"faces_all.txt\")\n", "with open(\"labels_all.txt\") as f:\n", "    labels = [line.rstrip() for line in f]\n", "# AUTOLAB_IGNORE_STOP"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Image Visualization \n", "\n", "It will be extremely valuable to be able to visualize the images and cluster means. So first you should create the following function, which plots a matrix of images in a grid structure. Each row of your data matrix is a vector of pixel values ranging from 0 to 255, as a grayscale value. \n", "\n", "### Specification\n", "* You can assume that the number of pixels is a square number\n", "* Use the matplotlib subplots to form a grid of images, and use `imshow` to plot the image. \n", "* For grading purposes, you should return a list of `matplotlib.image.AxesImage` objects. This is simply the result of calling the matplotlib `imshow` function. \n", "* The order of the returned objects should match the order of the matplotlib subplots. \n", "* You can assume that there are at least rows\\*cols entries in X. If there are more than that many, just plot the first rows\\*cols entries. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import math\n", "\n", "def plot_images(X, rows, cols):\n", "    \"\"\" Plots the centers in a (row by col) grid\n", "        Args: \n", "            centers (numpy 2D matrix) : matrix of centers, each row is a center\n", "            rows (int) : number of rows to plot in a grid\n", "            cols (int) : number of cols to plot in a grid\n", "        Returns: \n", "            (list) : list of matplotlib.image.AxesImage objects\n", "    \"\"\"\n", "    pass\n", "    \n", "# AUTOLAB_IGNORE_START\n", "plot_images(data, 10, 10)\n", "plt.show()\n", "# AUTOLAB_IGNORE_STOP"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## K-means clustering (KMeans++)\n", "The first task here is to implement k-means clustering, as seen on slides 12 and 23 of the unsupervised learning lecture. We will use this to attempt to cluster the images into something more meaningful. \n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Specification\n", "* Use np.argmin when assigning clusters to get a consistent result with the grading script. \n", "* You can refer to the above visualization to write your plotting function\n", "* You should initialize your centers by using the KMeans++ initialization described in the slides. This typically gives significantly better results. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["import math\n", "class KMeans:\n", "    def init_centers(self, X, k):\n", "        \"\"\" Initialize the starting k centers, using the KMeans++ algorithm. \n", "            Args: \n", "                X (numpy 2D matrix) : data matrix, each row is an example\n", "                k (float) : number of clusters\n", "            Return: \n", "                (numpy 2D matrix) : matrix of centers, each row is a center\n", "        \"\"\"\n", "        pass\n", "    \n", "    def assign_clusters(self, X, centers):\n", "        \"\"\" Given the data and the centers, assign clusters to all the examples. \n", "            Args: \n", "                X (numpy 2D matrix) : data matrix, each row is an example\n", "                centers (numpy 2D matrix) : matrix of centers, each row is a center\n", "            Return: \n", "                (numpy 2D matrix) : 1 hot encoding of cluster assignments for each example\n", "        \"\"\"\n", "        pass\n", "    \n", "    def compute_means(self, X, y):\n", "        \"\"\" Given the data and the cluster labels, compute the new cluster centers. \n", "            Args: \n", "                X (numpy 2D matrix) : data matrix, each row is an example\n", "                y (numpy 2D matrix) : 1 hot encoding of cluster assignments for each example\n", "            Return: \n", "                (numpy 2D matrix) : matrix of centers, each row is a center\n", "        \"\"\"\n", "        pass\n", "    \n", "    def train(self, X, centers, niters=20):\n", "        \"\"\" Args: \n", "                X (numpy 2D matrix) : data matrix, each row is an example\n", "                centers (numpy 2D matrix) : initial matrix of centers, each row is a center\n", "            Return: \n", "                (y, centers) : tuple of 1 hot encoding of cluster assignments for each example \n", "                               the resulting cluster centers\n", "        \"\"\"\n", "        pass\n", "     \n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Run your code on the given face images. Use the `plot25` function to visualize "]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": ["# AUTOLAB_IGNORE_START\n", "k = 25\n", "P = np.random.permutation(data.shape[0])\n", "KM = KMeans()\n", "# train and plot your centers\n", "# AUTOLAB_IGNORE_STOP"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Running K-means with 25 clusters on a <b>random</b> permutation gives the following result with our implementation: \n", "\n", "<img src=\"km.png\"> "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Cross-Entropy loss\n", "\n", "One way to to measure the quality of your clusters (if we have labels for the true cluster) is to measure the purity of a cluster: if a cluster contains all of the same label, then it is a good cluster. One such measure is the cross entropy loss, which you've already seen when learning about decision trees (and in the MNIST homework if you were in 688). If $p_i$ is the proportion of cluster elements belonging to the true cluster $i$, then:\n", "\n", "$$\\text{CrossEntropyLoss(cluster)}=-\\sum_{i\\in \\text{cluster}} p_i \\log p_i$$\n", "\n", "In other words, we take the proportions of all true clusters present in the estimated cluster, and calculate the cross entropy loss to see how good of a cluster it actually was. If everything in the cluster has the same label, then $p_i =1$ and so this sum is just 0, so 0 is a perfect score. The worst case is if the cluster is evenly distributed amongst $T$ clusters, and so $p_i = 1/T$ for $T$ clusters, resulting in a loss of $-\\log 1/T$. \n", "\n", "### Specification\n", "* Compute the cross entropy loss according to the above equation. \n", "* The sum should only be over true labels that are actually present in the cluster. \n", "* The total entropy is just the sum of the cross entropy loss of all the estimated clusters. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from collections import Counter\n", "\n", "def entropy(cluster_labels):\n", "    \"\"\" Calculate the entropy of a given cluster\n", "        Args: \n", "            cluster_labels (list) : list of true cluster labels assigned to a cluster\n", "        Return: \n", "            (float) : the cross entropy loss of this cluster\n", "    \"\"\"\n", "    pass\n", "    \n", "    \n", "def total_entropy(y, labels):\n", "    \"\"\" Compute the total cross entropy loss. \n", "        Args: \n", "            y (numpy 2D array) : one hot encoding of the estimated cluster labels\n", "            labels (list) : list of the true labels of each data point\n", "    \"\"\"\n", "    pass\n", "\n", "# AUTOLAB_IGNORE_START\n", "total_entropy(y, labels)\n", "# AUTOLAB_IGNORE_STOP"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The following code splits the dataset into 25 roughly even clusters. You should expect that the total entropy of the output of your K-means algorithm to do better than the total entropy of a random clustering. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# AUTOLAB_IGNORE_START\n", "y0 = np.zeros(len(P))\n", "size = int(math.ceil(float(len(P))/25))\n", "for i in range(k):\n", "    end = min((i+1)*size, len(P))\n", "    y0[P[i*size: end]] = i\n", "total_entropy(np.eye(len(P))[y0.astype(int),:], labels)\n", "# AUTOLAB_IGNORE_STOP"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Gaussian Mixture Model\n", "\n", "In this part, you will implement the softer cousin of K-means clustering: the Gaussian mixture model. For this problem, you should refer to the anomaly detection slides for the details of the algorithm. \n", "\n", "We have provided an implementation of the multivariate normal log PDF function that works on an array of samples. Note that scipy has one as well, but it can only calculate the log pdf for one example at a time and is not as efficient. \n", "\n", "### Specification\n", "* Again, you can initialize your centers using the first k random indices from the permutation. \n", "* You should add a diagonal regularization to your covariance matrix to avoid singularity issues. \n", "* Your covariance matrix should be intialized as the sample covariance of the entire data matrix plus regularization using the unbiased estimater. You can use `np.cov` for this. \n", "* You can initialize  `phi` as a uniform discrete distribution over the clusters. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["def mv_normal_log_pdf(X, mu, Sig):\n", "    return (-0.5*np.linalg.slogdet(2*np.pi*Sig)[1]\n", "     - 0.5*np.sum((X-mu)*(np.linalg.inv(Sig).dot((X-mu).T)).T,axis=1))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "scrolled": true}, "outputs": [], "source": ["class GaussianMixture:\n", "    def init_parameters(self, X, P, k, reg):\n", "        \"\"\" Initialize the parameters of means, covariances, and frequency counts. \n", "            Args: \n", "                X (numpy 2D matrix) : data matrix, each row is an example\n", "                P (numpy 2D matrix) : Random permutation vector\n", "                k (float) : number of clusters\n", "                reg (float) : regularization parameter for the covariance matrix\n", "            Returns: \n", "                mus (numpy 2D matrix) : matrix of initialzied means, chosen randomly by selection the first k elements of P\n", "                Sigmas (list) : list of 2D covariance matrices corresponding to each cluster\n", "                phi (numpy 1D vector) : vector of initialized frequencies\n", "        \"\"\"\n", "        pass\n", "\n", "    def Estep(self, X, mus, Sigmas, phi):\n", "        \"\"\" Perform an E step and return the resulting probabilities. \n", "            Args: \n", "                X (numpy 2D matrix) : data matrix, each row is an example\n", "                mus (numpy 2D matrix) : matrix of initialzied means, chosen randomly by selection the first k elements of P\n", "                Sigmas (list) : list of 2D covariance matrices corresponding to each cluster\n", "                phi (numpy 1D vector) : vector of initialized frequencies\n", "            Returns: \n", "                (numpy 2D matrix) : matrix of probabilities, where the i,jth entry corresponds to the probability of the\n", "                                    ith element being in the jth cluster. \n", "        \"\"\"\n", "        pass\n", "\n", "    def Mstep(self, ps, X, reg):\n", "        \"\"\" Initialize the parameters of means, covariances, and frequency counts. \n", "            Args: \n", "                ps (numpy 2D matrix) : matrix of probabilities, where the i,jth entry corresponds to the probability of the\n", "                                       ith element being in the jth cluster. \n", "                X (numpy 2D matrix) : data matrix, each row is an example\n", "                reg (float) : regularization parameter for the covariance matrix\n", "            Returns: \n", "                (mus, Sigmas, phi) : 3 tuple of matrix of initialzied means, chosen randomly by selection the first \n", "                                     k elements of P, a list of 2D covariance matrices corresponding to each cluster, \n", "                                     and a vector of initialized frequencies\n", "        \"\"\"\n", "        pass\n", "    \n", "    def train(self, X, mus, Sigmas, phi, niters = 5, reg=1e-4):\n", "        \"\"\" Train the model using the EM algorithm for a number of iterations. \n", "            Args: \n", "                X (numpy 2D matrix) : data matrix, each row is an example\n", "                mus (numpy 2D matrix) : matrix of initialzied means, chosen randomly by selection the first k elements of P\n", "                Sigmas (list) : list of 2D covariance matrices corresponding to each cluster\n", "                phi (numpy 1D vector) : vector of initialized frequencies\n", "                niters (int) : number of EM iterations to run\n", "            Returns: \n", "                (mus, Sigmas, phi) : 3 tuple of matrix of initialzied means, chosen randomly by selection the first \n", "                                     k elements of P, a list of 2D covariance matrices corresponding to each cluster, \n", "                                     and a vector of initialized frequencies \n", "        \n", "        \"\"\"\n", "        pass\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# AUTOLAB_IGNORE_START\n", "k = 5\n", "P = np.random.permutation(data.shape[0])\n", "GM = GaussianMixture()\n", "# train and plot your centers\n", "# AUTOLAB_IGNORE_STOP"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["The Gaussian mixture model takes a lot longer to train than the K-means model. You may also get better results initializing your clusters with the results from the K-means model. \n", "Running the Gaussian mixture model with an intialization from the K-means clusters, regularization $10^{-4}$, and 10 iterations gives the following Gaussian means (for k=5) on our implementation: \n", "\n", "<img src=\"gmm.png\">"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Face Classification Contest\n", "In general, facial classification in images is a difficult problem. Here, we will focus on a much simplified version of the problem: given images of faces with noise, can we classify a face as either George W. Bush or Colin Powell? We will designate George W. Bush as +1, and Colin Powell as -1. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": ["# Load the data\n", "# AUTOLAB_IGNORE_START\n", "data = np.loadtxt(\"faces_noisy.txt\")\n", "with open(\"labels_all.txt\") as f:\n", "    labels = [line.rstrip() for line in f]\n", "\n", "plot_images(data,10,10)\n", "plt.show()\n", "# AUTOLAB_IGNORE_STOP"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For your convenience, we've written some of the pipelining code for you which extracts only the images corresponding to George Bush and Colin Powell. It also randomly splits the resulting data into a train and a test set. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Extract the bush and powell images and create a random permutation\n", "# AUTOLAB_IGNORE_START\n", "bush_idxs = [i for i,l in enumerate(labels) if l==\"George_W_Bush\"]\n", "powell_idxs = [i for i,l in enumerate(labels) if l==\"Colin_Powell\"]\n", "all_data = data[bush_idxs + powell_idxs]\n", "all_labels = np.hstack([np.ones(len(bush_idxs)), -np.ones(len(powell_idxs))])\n", "P = np.random.permutation(len(all_labels))\n", "# AUTOLAB_IGNORE_STOP"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Split into a training and a test set\n", "# AUTOLAB_IGNORE_START\n", "split = 400\n", "tr_idx, te_idx = P[:split], P[split:]\n", "data_tr, data_te = all_data[tr_idx], all_data[te_idx]\n", "labels_tr, labels_te = all_labels[tr_idx], all_labels[te_idx]\n", "# AUTOLAB_IGNORE_STOP"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### SVM refresher\n", "An easy first attempt at this problem is to just throw the data into an SVM. You can copy your SVM from the previous homework here, or use the one in the reference solution (which implements everything in 1-2 lines). What happens when you try to just straight up use an SVM? "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your SVM implementation\n", "class SVM:\n", "    pass\n", "# AUTOLAB_IGNORE_START\n", "svm = SVM(data_tr, labels_tr, 1e-4)\n", "svm.train(verbose=True, niters=50)\n", "(svm.predict(data_te)==labels_te).mean()\n", "# AUTOLAB_IGNORE_STOP"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Spoiler alert: the SVM gets stuck and ends up just predicting 1 for everything, so every image is George W. Bush! This is because noisy images are even more nonlinear than normal images, and so the SVM has no hope at separating the two classes with a hyperplane. \n", "\n", "Without the noise (you can try this yourself), the the dataset is so nicely processed that Bush and Powell are actually linearly separable, but in real world images this is rarely ever the case. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Contest details\n", "\n", "For this homework, your goal is to achieve at least 80% accuracy on the classification problem between George W. Bush and Colin Powell. You will need to write an end-to-end classifier that takes in training inputs, training outputs, and testing inputs. It should return predicted outputs for the testing outputs. \n", "\n", "Note: this contest is not for extra credit. The points for implementing the predict_faces function count towards regular points on the assignment. The contest leaderboard will be ranked according to your accuracy. \n", "\n", "\n", "### Baseline approach\n", "\n", "In lecture, Zico mentioned that a common technique when using RBF features is to use KMeans to select RBF centers, which help make nonlinear decision boundaries.  With a proper choice of parameters, this can do quite well on this task. Using an SVM with RBF features as described in lecture (with the median trick) and KMeans++, you can achieve full credit on this problem (our implementation achieves 81-82% accuracy).  \n", "\n", "However, similar to the last contest, you are free to do whatever you'd like and do not have to adhere to this solution. We have tried multiple other methodologies, which are also able to break the 80% barrier, and some of which are quite simple. With proper domain knowledge, you could even do this with just feature engineering and vanilla SVM!\n", "\n", "### Specification: \n", "* Your function will receive the a labeled training set, and an unlabeled testing set, in the same format as that provided in this notebook. \n", "* There are no restrictions other than 1) you cannot use sklearn (we found it to be largely unhelpful, so you aren't missing much) and 2) you are restricted to the computing capabilities on Autolab. \n", "* You will receive full credit if you get above an 80% accuracy score. You will receive a scaled score for every % point you get above predicting the most frequent training example (e.g. predicting 1 for everything). \n", "* We've taken care of almost all the plumbing code for you in this assignment, so you just need to worry about predict_faces. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": ["              \n", "def predict_faces(pixels, labels, pixels_te):\n", "    \"\"\" Given some pixels and labels as training data, predict the labels of the testing set. \n", "        Args: \n", "            pixels (Numpy 2D array) : Array of pixels for each training example\n", "            labels (Numpy 1D vector) : Vector of class labels for each training example\n", "            pixels_te (Numpy 2D array) : Array of pixels for each testing example\n", "        Return:\n", "            (Numpy 1D vector) : Vector of predicted class labels for each testing example\n", "    \"\"\"\n", "    pass\n", "\n", "# AUTOLAB_IGNORE_START\n", "p_te = predict_faces(data_tr, labels_tr, data_te)\n", "print((labels_te==p_te).mean())\n", "# AUTOLAB_IGNORE_STOP"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python [py36]", "language": "python", "name": "Python [py36]"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.3"}}, "nbformat": 4, "nbformat_minor": 1}