{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Introduction\n", "\n", "As part of your final assignment in this course, this notebook will cover the full end to end data science pipeline, from the initial visualizations to the final predictions.  "]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["import matplotlib\n", "matplotlib.use(\"svg\")\n", "# AUTOLAB_IGNORE_START\n", "%matplotlib inline\n", "# AUTOLAB_IGNORE_STOP\n", "import matplotlib.pyplot as plt\n", "plt.style.use(\"ggplot\")\n", "import numpy as np"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Installation\n", "\n", "You will need to install PyTorch for this assignment. At this point, installation of libraries should be relatively painless however you should try to install early on.\n", "\n", "You should be able to install PyTorch using conda. \n", "\n", "    conda install pytorch torchvision -c pytorch\n", "\n", "If you run into trouble, you can check out their documentation located at http://pytorch.org/."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Image Dataset\n", "\n", "Before we get to using the neural networks, as you are probably used to by now, we must fetch and process the data. For this assignment, you will be using the Oxford-IIIT pet dataset. Go download it from here: \n", "\n", "    http://www.robots.ox.ac.uk/~vgg/data/pets/\n", "    \n", "It is a non-trivial download of ~800MB containing a collection of cats and dogs, annotated with their breed, head ROI (region of interest), and a pixel level segmentation. First we will need to canonicalize the images. We will do so using the `pillow` package, which should be already included in your Anaconda installations. If you'd like to know more, you can view the tutorial here: https://pillow.readthedocs.io/en/5.1.x/handbook/tutorial.html."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["from PIL import Image\n", "\n", "def plot_image(im):\n", "    plt.imshow(np.asarray(im))\n", "\n", "# AUTOLAB_IGNORE_START\n", "im_cat = Image.open(\"images/Abyssinian_1.jpg\")\n", "print(im_cat.format, im_cat.size, im_cat.mode)\n", "plot_image(im_cat)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "scrolled": true}, "outputs": [], "source": ["im_dog = Image.open(\"images/english_cocker_spaniel_198.jpg\")\n", "plot_image(im_dog)\n", "# AUTOLAB_IGNORE_STOP"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Canonicalizing Images [5pts]\n", "Suppose we tried to use the raw pixel data, similar to the unsupervised learning notebook with the Bush and Powell faces. You'll immediately realize a problem: all the images are of different shapes and sizes (e.g. the above two pictures)! This means we cannot throw them straight away into an SVM, for example. Without using fairly advanced techniques, we will have to do some processing or feature extraction to get a consistent number of features per example. \n", "\n", "Here we will take a simple approach: We have done some inspection for you and noticed that every picture has at least 100 pixels in either width or height. Thus, we will crop each image down to be a square picture, then scale it down to 100x100 pixels. \n", "\n", "### Specification\n", "1. First, crop the image to be a square by reducing one dimension to be the same size as the other. You should center your cropping as much as possible (e.g. crop the same number of pixels from the left as from the right). If you must crop an odd number of pixels, then crop an extra pixel from the right or from the bottom. \n", "2. Second, resize the image to be 100 by 100 pixels. You should use the `Image.ANTIALIAS` filter for the resample parameter. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "scrolled": false}, "outputs": [], "source": ["def crop_and_scale_image(im):\n", "    \"\"\" Crops and scales a given image. \n", "        Args: \n", "            im (PIL Image) : image object to be cropped and scaled\n", "        Returns: \n", "            (PIL Image) : cropped and scaled image object\n", "    \"\"\"\n", "    pass\n", "\n", "# AUTOLAB_IGNORE_START  \n", "plot_image(crop_and_scale_image(im_cat))\n", "# AUTOLAB_IGNORE_STOP"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Train / Validate / Test splits for large datasets\n", "Next we will load the data and perform our usual data split. However, the image dataset is **very large**. So large, that if you try to load every image and process them at in batch, your computer may run out of memory (mine did). Thus, initially we will work with the filenames until it is a manageable size. This code assumes that your downloaded images exist in a folder `images/`, and creates an array of filepaths to each image. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# AUTOLAB_IGNORE_START  \n", "import os\n", "dname = \"images/\"\n", "im_paths = np.array([dname+fname for fname in os.listdir(dname) if fname.endswith(\".jpg\")])\n", "P = np.random.permutation(len(im_paths))\n", "\n", "split = 2000\n", "fnames_tr, fnames_va, fnames_te = im_paths[P[:split]], im_paths[P[split:2*split]], im_paths[P[2*split:]]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["plt.figure(figsize=(20,20))\n", "for i,fname in enumerate(fnames_tr[:100]):\n", "    plt.subplot(10,10,i+1)\n", "    plt.axis('off')\n", "    plot_image(crop_and_scale_image(Image.open(fname)))\n", "# AUTOLAB_IGNORE_STOP"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Filename Parsing [2+3pts]\n", "\n", "We will need to extract the breeds and inputs for our VGG network from these files. Implement the following two helper functions to do this. \n", "\n", "### Specification\n", "* Extract the full breed name from each filename, which is the part of the filename not including the number and the extension. See the example output for an example. \n", "* Using your `crop_and_scale_image` function from earlier, generate the image data matrix that is to be input into VGG. \n", "* Note that VGG requires its input dimension to be in a slightly different order than that returned by Pillow. Use `np.rollaxis` to rotate it until in the proper order. \n", "* Some of the images are not in RGB format. You will have to convert them to RGB. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["def fname_to_breed(fname):\n", "    \"\"\" Extracts the breed from a filename\n", "        Args: \n", "            fname (string) : the filename to be parsed\n", "        Returns: \n", "            (string) : the breed extracted from the filename\n", "    \"\"\"\n", "    pass\n", "def fname_to_vgg_input(fname):\n", "    \"\"\" Creates the input for a VGG network from the filename \n", "        Args: \n", "            fname (string) : the filename to be parsed\n", "        Returns: \n", "            (numpy ndarray) : the array to be passed into the VGG network as a single example\n", "    \"\"\"\n", "    pass\n", "\n", "# AUTOLAB_IGNORE_START\n", "print(fname_to_breed(\"images/english_cocker_spaniel_144.jpg\"))\n", "print(fname_to_vgg_input(\"images/Abyssinian_1.jpg\"))\n", "# AUTOLAB_IGNORE_STOP"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Our implementation gets the following output: \n", "     \n", "    english_cocker_spaniel\n", "    [[[59 61 61 ..., 74 72 71]\n", "      [58 59 61 ..., 76 72 70]\n", "      [59 62 65 ..., 75 71 70]\n", "      ..., \n", "      [21 21 22 ..., 23 25 26]\n", "      [18 19 19 ..., 23 24 23]\n", "      [16 17 18 ..., 22 21 22]]\n", "\n", "     [[70 72 72 ..., 87 85 82]\n", "      [69 69 72 ..., 89 84 81]\n", "      [69 71 74 ..., 86 83 82]\n", "      ..., \n", "      [27 26 28 ..., 29 31 32]\n", "      [25 27 27 ..., 29 30 29]\n", "      [23 25 26 ..., 28 27 28]]\n", "\n", "     [[53 55 55 ..., 69 67 65]\n", "      [52 52 55 ..., 71 66 64]\n", "      [53 54 57 ..., 69 65 62]\n", "      ..., \n", "      [17 19 20 ..., 19 21 22]\n", "      [17 16 17 ..., 19 20 19]\n", "      [15 14 15 ..., 18 17 18]]]\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## VGG [10pts]\n", "\n", "As mentioned in lecture, one of the common ways to use deep learning is to automatically learn features. For this problem, we will use a pretrained [VGG network](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) to create useful features. Tuning and training your own custom neural networks takes a significant amount of time and effort (much more so than running SVM with RBF features), so we will use the pretrained network VGG-16 in the PyTorch deep learning framework. You will find the documentation here helpful: http://pytorch.org/docs/master/torchvision/models.html. \n", "\n", "\n", "### Specification\n", "* Load the pretrained VGG network using PyTorch"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["import torch\n", "import torchvision.models as models\n", "from torch.autograd import Variable\n", "\n", "def VGG_16():\n", "    \"\"\" Loads a pretrained VGG network. \n", "        Returns: \n", "            (pytorch VGG model) : the VGG-16 model\n", "    \"\"\"\n", "\n", "# AUTOLAB_IGNORE_START\n", "truncated_vgg = VGG_16()\n", "# AUTOLAB_IGNORE_STOP"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Converting files to features and labels [2+3pts]\n", "Now we have all the parts to convert our image files to features and labels. Implement the following two functions that turn filenames into labels and filenames to features. \n", "\n", "### Specification\n", "* You should take all the breeds and sort them in alphabetical order. Then, assign the a label of 0 to the first, and so on, so that the last breed in alphabetical order has a label of 36. \n", "* The features are the flattened output of the last layer of the VGG network."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["def fnames_to_labels(fnames):\n", "    \"\"\" Given a list of filenames, generate the corresponding array of labels\n", "        Args: \n", "            fnames (list) : A list of filenames\n", "        Returns: \n", "            (numpy ndarray) : a 1D array of numerical labels\n", "    \"\"\"\n", "    pass\n", "\n", "# AUTOLAB_IGNORE_START\n", "fnames_to_labels(fnames_tr[:10])\n", "# AUTOLAB_IGNORE_STOP"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["def fnames_to_features(fnames, vgg):\n", "    \"\"\" Given a list of filenames and a VGG16 model, generate the corresponding array of VGG features\n", "        Args: \n", "            fnames (list) : A list of filenames\n", "            vgg (pytorch model) : a pretrained VGG16 model\n", "        Returns: \n", "            (pytorch Variable) : a (m x 4608)-dimensional Variable of features generated from the VGG model,\n", "                                 where m is the number of filenames\n", "    \"\"\"\n", "    pass\n", "\n", "# AUTOLAB_IGNORE_START\n", "fnames_to_features(fnames_tr[:10], truncated_vgg)\n", "# AUTOLAB_IGNORE_STOP"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## And now we wait... just kidding. \n", "Now, you can run this on the entirety of the training, validation, and test set... but wait. Our machines take about 2 seconds per example to generate VGG examples without any GPU acceleration, which means that processing all the images will be a several hour endeavor (about 8 hours on our machines)! To save you some time, we've already run this on a random training and validation set (2k examples each), in `X_tr.txt` and `y_va.txt`. You can find their labels in `y_tr.txt` and `y_va.txt`. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# AUTOLAB_IGNORE_START\n", "X_tr = np.loadtxt(\"X_tr.txt\")\n", "X_va = np.loadtxt(\"X_va.txt\")\n", "# AUTOLAB_IGNORE_STOP"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# AUTOLAB_IGNORE_START\n", "y_tr = np.loadtxt(\"y_tr.txt\", dtype=int)\n", "y_va = np.loadtxt(\"y_va.txt\", dtype=int)\n", "# AUTOLAB_IGNORE_STOP"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Pet breed classification from VGG features [8pts]\n", "\n", "And now for the final task: you will now classify the pets by breed using the generated VGG features. Your goal here is to get at least 50% classification accuracy on the pets dataset. You will be given the 4k random examples above to build your model (the train and validation datasets), and we will evaluate your model on a holdout test set. \n", "\n", "### Baseline approach\n", "\n", "The baseline approach here is to use yet another neural network. A simple network that isn't too large can train to about 55% accuracy in a minute on our machines. Try different configurations with different learners and architectures. You must get at least 50% accuracy to receive credit for this problem. You can use the training/validation split that we gave you to estimate the performance of your model on autolab prior to submission. \n", "\n", "### Contest rules\n", "\n", "We've increased the time alotted for autograding to 6 minutes per submission. Other than that, there are no restrictions on your models for this contest (yes, you can use sklearn). You may use any library on Autolab. Use everything you've learned in this course to create the best pet classifier you can!\n", "\n", "Note: this contest is not for extra credit. The points for implementing the predict_from_features function count towards regular points on the assignment.\n", "\n", "### Specification\n", "* We will give your model 4k training examples, and evaluate its accuracy on a holdout set. \n", "* Scores of 50% or more will receive full homework credit."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["import time\n", "\n", "def predict_from_features(X, y, X_te):\n", "    \"\"\" Given labels and VGG features, predict the breed of the testing set. \n", "    Args: \n", "        X (numpy ndarray) : 2D array of VGG features, each row is a set of features for a single example\n", "        y (numpy ndarray) : 1D array of labels corresponding to the features in X\n", "        X_te (numpy ndarray) : 2D array of VGG features for unlabeled examples\n", "    Returns: \n", "        (numpy ndarray) 1D array of predicted labels for the unlabeled examples in X_te\n", "    \"\"\"\n", "    pass\n", "\n", "# AUTOLAB_IGNORE_START\n", "start = time.time()\n", "y_p = predict_from_features(X_tr, y_tr, X_va)\n", "end = time.time()\n", "print(\"Validation accuracy: {} in {} seconds\".format(np.mean(y_p==y_va), end-start))\n", "\n", "# AUTOLAB_IGNORE_STOP"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "scrolled": true}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "scrolled": true}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "scrolled": true}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python [py36]", "language": "python", "name": "Python [py36]"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.3"}}, "nbformat": 4, "nbformat_minor": 1}