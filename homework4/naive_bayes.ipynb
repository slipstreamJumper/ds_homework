{"nbformat_minor": 1, "nbformat": 4, "cells": [{"execution_count": null, "cell_type": "code", "source": ["import pandas as pd\n", "import numpy as np\n", "import scipy\n", "from scipy import stats"], "outputs": [], "metadata": {"collapsed": true}}, {"source": ["# Naive Bayes Classifier [30 pts]\n", "\n", "## Introduction\n", "Naive Bayes is a class of simple classifiers based on the Bayes' Rule and strong (or naive) independence assumptions between features. In this problem, you will implement a Naive Bayes Classifier for the Census Income Data Set from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/) (which is a good website to browse through for datasets).\n", "\n", "## Dataset Description\n", "The dataset consists 32561 instances, each representing an individual. The goal is to predict whether a person makes over 50K a year based on the values of 14 features. The features, extracted from the 1994 Census database, are a mix of continuous and discrete attributes. These are enumerated below:\n", "\n", "#### Continuous (real-valued) features\n", "- age\n", "- final_weight (computed from a number of attributes outside of this dataset; people with similar demographic attributes have similar values)\n", "- education_num\n", "- capital_gain\n", "- capital_loss\n", "- hours_per_week\n", "\n", "#### Categorical (discrete) features \n", "- work_class: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked\n", "- education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool\n", "- marital_status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse\n", "- occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces\n", "- relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n", "- race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black\n", "- sex: Female, Male\n", "- native_country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands"], "cell_type": "markdown", "metadata": {}}, {"source": ["## Q1. Input preparation [2 pts]\n", "First, you need to load in the above data, provided to you as a CSV file. As the data is from UCI repository, it is already quite clean. However, some instances contain missing values (represented as ? in the CSV file) and these have to be discarded from the training set. Also, replace the `income` column with `label`, which is 1 if `income` is `>50K` and 0 otherwise."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["def load_data(file_name):\n", "    \"\"\" loads and processes data in the manner specified above\n", "    Inputs:\n", "        file_name (str): path to csv file containing data\n", "    Outputs:\n", "        pd.DataFrame: processed dataframe\n", "    \"\"\"\n", "    pass\n", "\n", "# AUTOLAB_IGNORE_START\n", "df = load_data('census.csv')\n", "# AUTOLAB_IGNORE_STOP"], "outputs": [], "metadata": {}}, {"source": ["Our reference code yields the following output (pay attention to the index):\n", "```python\n", ">>> print(df.dtypes)\n", "age                int64\n", "work_class        object\n", "final_weight       int64\n", "education         object\n", "education_num      int64\n", "marital_status    object\n", "occupation        object\n", "relationship      object\n", "race              object\n", "sex               object\n", "capital_gain       int64\n", "capital_loss       int64\n", "hours_per_week     int64\n", "native_country    object\n", "label              int64\n", "dtype: object\n", "    \n", ">>> print(df.tail())\n", "       age    work_class  final_weight   education  education_num  \\\n", "30157   27       Private        257302  Assoc-acdm             12   \n", "30158   40       Private        154374     HS-grad              9   \n", "30159   58       Private        151910     HS-grad              9   \n", "30160   22       Private        201490     HS-grad              9   \n", "30161   52  Self-emp-inc        287927     HS-grad              9   \n", "\n", "           marital_status         occupation relationship   race     sex  \\\n", "30157  Married-civ-spouse       Tech-support         Wife  White  Female   \n", "30158  Married-civ-spouse  Machine-op-inspct      Husband  White    Male   \n", "30159             Widowed       Adm-clerical    Unmarried  White  Female   \n", "30160       Never-married       Adm-clerical    Own-child  White    Male   \n", "30161  Married-civ-spouse    Exec-managerial         Wife  White  Female   \n", "\n", "       capital_gain  capital_loss  hours_per_week native_country  label  \n", "30157             0             0              38  United-States      0  \n", "30158             0             0              40  United-States      1  \n", "30159             0             0              40  United-States      0  \n", "30160             0             0              20  United-States      0  \n", "30161         15024             0              40  United-States      1  \n", ">>> print(len(df))\n", "30162\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["## Overview of Naive Bayes classifier\n", "Let $X_1, X_2, \\ldots, X_k$ be the $k$ features of a dataset, with class label given by the variable $y$. A probabilistic classifier assigns the most probable class to each instance $(x_1,\\ldots,x_k)$, as expressed by\n", "$$ \\hat{y} = \\arg\\max_y P(y\\ |\\ x_1,\\ldots,x_k) $$\n", "\n", "Using Bayes' theorem, the above *posterior probability* can be rewritten as\n", "$$ P(y\\ |\\ x_1,\\ldots,x_k) = \\frac{P(y) P(x_1,\\ldots,x_n\\ |\\ y)}{P(x_1,\\ldots,x_k)} $$\n", "where\n", "- $P(y)$ is the prior probability of the class\n", "- $P(x_1,\\ldots,x_k\\ |\\ y)$ is the likelihood of data under a class\n", "- $P(x_1,\\ldots,x_k)$ is the evidence for data\n", "\n", "Naive Bayes classifiers assume that the feature values are conditionally independent given the class label, that is,\n", "$ P(x_1,\\ldots,x_n\\ |\\ y) = \\prod_{i=1}^{k}P(x_i\\ |\\ y) $. This strong assumption helps simplify the expression for posterior probability to\n", "$$ P(y\\ |\\ x_1,\\ldots,x_k) = \\frac{P(y) \\prod_{i=1}^{k}P(x_i\\ |\\ y)}{P(x_1,\\ldots,x_k)} $$\n", "\n", "For a given input $(x_1,\\ldots,x_k)$, $P(x_1,\\ldots,x_k)$ is constant. Hence, we can simplify omit the denominator replace the equality sign with proportionality as follows:\n", "$$ P(y\\ |\\ x_1,\\ldots,x_k) \\propto P(y) \\prod_{i=1}^{k}P(x_i\\ |\\ y) $$\n", "\n", "Thus, the class of a new instance can be predicted as $\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{k}P(x_i\\ |\\ y)$. Here, $P(y)$ is commonly known as the **class prior** and $P(x_i\\ |\\ y)$ termed **feature predictor**. The rest of the assignment deals with how each of these $k+1$ probability distributions -- $P(y), P(x_1\\ |\\ y), \\ldots, P(x_k\\ |\\ y)$ -- are estimated from data.\n", "\n", "\n", "**Note**: Observe that the computation of the final expression above involve multiplication of $k+1$ probability values (which can be really low). This can lead to an underflow of numerical precision. So, it is a good practice to use a log transform of the probabilities to avoid this underflow.\n", "\n", "** TL;DR ** Your final take away from this cell is the following expression:\n", "$$\\hat{y} = \\arg\\max_y \\underbrace{\\log P(y)}_{log-prior} + \\underbrace{\\sum_{i=1}^{k} \\log P(x_i\\ |\\ y)}_{log-likelihood}$$\n", "\n", "Each term in the sum for log-likelihood can be regarded a partial log-likelihood based on a particular feature alone."], "cell_type": "markdown", "metadata": {}}, {"source": ["## Feature Predictor\n", "The beauty of a Naive Bayes classifier lies in the fact we can mix-and-match different likelihood models for each feature predictor according to the prior knowledge we have about it and these models can be varied independent of each other. For example, we might know that $P(X_i|y)$ for some continuous feature $X_i$ is normally distributed or that $P(X_i|y)$ for some categorical feature follows multinomial distribution. In such cases, we can directly plugin the pdf/pmf of these distributions in place of $P(x_i\\ |\\ y)$.\n", "\n", "In this assignment, you will be using two classes of likelihood models:\n", "- Gaussian model, for continuous real-valued features (parameterized by mean $\\mu$ and variance $\\sigma$)\n", "- Categorical model, for discrete features (parameterized by $\\mathbf{p} = <p_0,\\ldots,p_{l-1}>$, where $l$ is the number of values taken by this categorical feature)\n", "\n", "You need to implement a predictor class for each likelihood model. Each predictor should implement two functionalities:\n", "- **Parameter estimation `init()`**: Learn parameters of the likelihood model using MLE (Maximum Likelihood Estimator). You need to keep track of $k$ sets of parameters, one for each class, *in the increasing order of class id, i.e., mu[i] indicates the mean of class $i$ in the Gaussian Predictor*.\n", "- **Partial Log-Likelihood computation for *this* feature `partial_log_likelihood()`**: Use the learnt parameters to compute the probability (density/mass for continuous/categorical features) of a given feature value. Report np.log() of this value.\n", "\n", "The parameter estimation is for the conditional distributions $P(X|Y)$. Thus, while estimating parameters for a specific class (say class 0), you will use only those data points in the training set (or rows in the input data frame) which have class label 0."], "cell_type": "markdown", "metadata": {}}, {"source": ["## Q2. Gaussian Feature Predictor [8pts]\n", "The Guassian distribution is characterized by two parameters - mean $\\mu$ and standard deviation $\\sigma$:\n", "$$ f_Z(z) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp{(-\\frac{(z-\\mu)^2}{2\\sigma^2})} $$\n", "\n", "Given $n$ samples $z_1, \\ldots, z_n$ from the above distribution, the MLE for mean and standard deviation are:\n", "$$ \\hat{\\mu} = \\frac{1}{n} \\sum_{j=1}^{n} z_j $$\n", "\n", "$$ \\hat{\\sigma} = \\sqrt{\\frac{1}{n} \\sum_{j=1}^{n} (z_j-\\hat{\\mu})^2} $$\n", "\n", "`scipy.stats.norm` would be helpful."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["class GaussianPredictor:\n", "    \"\"\" Feature predictor for a normally distributed real-valued, continuous feature.\n", "        Attributes: \n", "            mu (array_like) : vector containing per class mean of the feature\n", "            sigma (array_like): vector containing per class std. deviation of the feature\n", "    \"\"\"\n", "    # feel free to define and use any more attributes, e.g., number of classes, etc\n", "    def __init__(self, x, y) :\n", "        \"\"\" initializes the predictor statistics (mu, sigma) for Gaussian distribution\n", "        Inputs:\n", "            x (array_like): feature values (continuous)\n", "            y (array_like): class labels (0,...,k-1)\n", "        \"\"\"\n", "        self.k = len(y.unique())\n", "        self.mu = np.zeros(self.k)\n", "        self.sigma = np.zeros(self.k)\n", "    \n", "    def partial_log_likelihood(self, x):\n", "        \"\"\" log likelihood of feature values x according to each class\n", "        Inputs:\n", "            x (array_like): vector of feature values\n", "        Outputs:\n", "            (array_like): matrix of log likelihood for this feature alone\n", "        \"\"\"\n", "        pass\n", "\n", "# AUTOLAB_IGNORE_START\n", "f = GaussianPredictor(df['age'], df['label'])\n", "# AUTOLAB_IGNORE_STOP"], "outputs": [], "metadata": {}}, {"source": ["Our reference code gives the following output:\n", "```python\n", ">>> f.mu\n", "array([ 36.60806039  43.95911028])\n", ">>> f.sigma\n", "array([ 13.46433407  10.2689489 ])\n", ">>> f.partial_log_likelihood([43,40,100,10])\n", "array([[ -3.63166766,  -3.2524249 ],\n", "       [ -3.55071473,  -3.32238449],\n", "       [-14.60226337, -18.13920716],\n", "       [ -5.47164304,  -8.71608989]])\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["## Q3. Categorical Feature Predictor [8pts]\n", "The categorical distribution with $l$ categories $\\{0,\\ldots,l-1\\}$ is characterized by parameters $\\mathbf{p} = (p_0,\\dots,p_{l-1})$:\n", "$$ P(z; \\mathbf{p}) = p_0^{[z=0]}p_1^{[z=1]}\\ldots p_{l-1}^{[z=l-1]} $$\n", "\n", "where $[z=t]$ is 1 if $z$ is $t$ and 0 otherwise.\n", "\n", "Given $n$ samples $z_1, \\ldots, z_n$ from the above distribution, the smoothed-MLE for each $p_t$ is:\n", "$$ \\hat{p_t} = \\frac{n_t + \\alpha}{n + l\\alpha} $$\n", "\n", "where $n_t = \\sum_{j=1}^{n} [z_j=t]$, i.e., the number of times the label $t$ occurred in the sample. The smoothing is done to avoid zero-count problem (similar in spirit to $n$-gram model in NLP).\n", "\n", "**Note:** You have to learn the number of classes and the number and value of labels from the data. We might be testing your code on a different categorical feature."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["class CategoricalPredictor:\n", "    \"\"\" Feature predictor for a categorical feature.\n", "        Attributes: \n", "            p (dict) : dictionary of vector containing per class probability of a feature value;\n", "                    the keys of dictionary should exactly match the values taken by this feature\n", "    \"\"\"\n", "    # feel free to define and use any more attributes, e.g., number of classes, etc\n", "    def __init__(self, x, y, alpha=1) :\n", "        \"\"\" initializes the predictor statistics (p) for Categorical distribution\n", "        Inputs:\n", "            x (array_like): feature values (categorical)\n", "            y (array_like): class labels (0,...,k-1)\n", "        \"\"\"\n", "        self.p = {}\n", "    \n", "    def partial_log_likelihood(self, x):\n", "        \"\"\" log likelihood of feature values x according to each class\n", "        Inputs:\n", "            x (array_like): vector of feature values\n", "        Outputs:\n", "            (array_like): matrix of log likelihood for this feature\n", "        \"\"\"\n", "        pass\n", "# AUTOLAB_IGNORE_START\n", "f = CategoricalPredictor(df['sex'], df['label'])\n", "# AUTOLAB_IGNORE_STOP"], "outputs": [], "metadata": {}}, {"source": ["Our reference code gives the following output:\n", "```python\n", ">>> f.p\n", "{'Female': array([ 0.38272422,  0.1482024 ]),\n", " 'Male': array([ 0.61727578,  0.8517976 ])}\n", ">>> f.partial_log_likelihood(['Male','Female','Male'])\n", "array([[-0.48243939 -0.16040634]\n", "       [-0.96044059 -1.90917639]\n", "       [-0.48243939 -0.16040634]])\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["## Q4. Putting things together [10pts]\n", "It's time to put all the feature predictors together and do something useful! You will implement two functions in the following class.\n", "\n", "1. **__init__()**: Compute the log prior for each class and initialize the feature predictors (based on feature type). The smoothed prior for class $t$ is given by\n", "$$ prior(t) = \\frac{n_t + \\alpha}{n + k\\alpha} $$\n", "where $n_t = \\sum_{j=1}^{n} [y_j=t]$, i.e., the number of times the label $t$ occurred in the sample. \n", "\n", "2. **predict()**: For each instance and for each class, compute the sum of log prior and partial log likelihoods for all features. Use it to predict the final class label. Break ties by predicting the class with lower id.\n", "\n", "**Note:** Your implementation should not assume anything about the schema of the input data frame or the number of classes. The only guarantees you have are: (1) there will be a `label` column with values $0,\\ldots,k-1$ for some $k$. And the datatypes of the columns will be either `object` (string, categorical) or `int64` (integer)."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["class NaiveBayesClassifier:\n", "    \"\"\" Naive Bayes classifier for a mixture of continuous and categorical attributes.\n", "        We use GaussianPredictor for continuous attributes and MultinomialPredictor for categorical ones.\n", "        Attributes:\n", "            predictor (dict): model for P(X_i|Y) for each i\n", "            log_prior (array_like): log P(Y)\n", "    \"\"\"\n", "    # feel free to define and use any more attributes, e.g., number of classes, etc\n", "    def __init__(self, df, alpha=1):\n", "        \"\"\"initializes predictors for each feature and computes class prior\n", "        Inputs:\n", "            df (pd.DataFrame): processed dataframe, without any missing values.\n", "        \"\"\"\n", "        pass\n", "\n", "    def predict(self, x):\n", "        \"\"\" predicts label for input instances from log_prior and partial_log_likelihood of feature predictors\n", "        Inputs:\n", "            x (pd.DataFrame): processed dataframe, without any missing values and without class label.\n", "        Outputs:\n", "            (array_like): array of predicted class labels (0,..,k-1)\n", "        \"\"\"\n", "        pass\n", "\n", "# AUTOLAB_IGNORE_START\n", "c = NaiveBayesClassifier(df, 0)\n", "y_pred = c.predict(df)\n", "# AUTOLAB_IGNORE_STOP"], "outputs": [], "metadata": {}}, {"source": ["Our reference code gives the following output:\n", "```python\n", ">>> c.log_prior\n", "array([-0.28624642, -1.39061374])\n", ">>> c.predictor\n", "{'age': <__main__.GaussianPredictor instance at 0x115edbcb0>,\n", " 'capital_gain': <__main__.GaussianPredictor instance at 0x114c19320>,\n", " 'capital_loss': <__main__.GaussianPredictor instance at 0x114c19998>,\n", " 'education': <__main__.CategoricalPredictor instance at 0x114c04638>,\n", " 'education_num': <__main__.GaussianPredictor instance at 0x114c04f38>,\n", " 'final_weight': <__main__.GaussianPredictor instance at 0x114c045a8>,\n", " 'hours_per_week': <__main__.GaussianPredictor instance at 0x114c19ef0>,\n", " 'marital_status': <__main__.CategoricalPredictor instance at 0x114c047a0>,\n", " 'native_country': <__main__.CategoricalPredictor instance at 0x114c19f80>,\n", " 'occupation': <__main__.CategoricalPredictor instance at 0x114c195a8>,\n", " 'race': <__main__.CategoricalPredictor instance at 0x114c19bd8>,\n", " 'relationship': <__main__.CategoricalPredictor instance at 0x114c19a28>,\n", " 'sex': <__main__.CategoricalPredictor instance at 0x114c19d40>,\n", " 'work_class': <__main__.CategoricalPredictor instance at 0x115edbb90>}\n", ">>> c.predictor['hours_per_week'].mu\n", "array([ 39.34859186  45.70657965])\n", ">>> c.predictor['hours_per_week'].sigma\n", "array([ 11.95051037  10.73627157])\n", ">>> c.predictor['work_class'].p\n", "{'Federal-gov': array([ 0.02551426,  0.04861481]),\n", " 'Local-gov': array([ 0.0643595 ,  0.08111348]),\n", " 'Private': array([ 0.7685177,  0.6494406]),\n", " 'Self-emp-inc': array([ 0.02092346,  0.07991476]),\n", " 'Self-emp-not-inc': array([ 0.07879403,  0.09509856]),\n", " 'State-gov': array([ 0.04127306,  0.04581779]),\n", " 'Without-pay': array([ 0.00061799,  0.        ])}\n", ">>> y_pred.shape\n", "(30162,)\n", ">>> y_pred\n", "array([0, 0, 0, ..., 0, 0, 1])\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["## Q5. Evaluation - Error rate [2pts]\n", "If a classifier makes $n_e$ errors on a data of size $n$, its error rate is $n_e/n$. Fill the following function, to evaluate your classifier."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["def evaluate(y_hat, y):\n", "    \"\"\" Evaluates classifier predictions\n", "        Inputs:\n", "            y_hat (array_like): output from classifier\n", "            y (array_like): true class label\n", "        Output:\n", "            (double): error rate as defined above\n", "    \"\"\"\n", "    pass\n", "\n", "# AUTOLAB_IGNORE_START\n", "evaluate(y_pred, df['label'])\n", "# AUTOLAB_IGNORE_STOP"], "outputs": [], "metadata": {}}, {"source": ["Our implementation yields 0.17240236058616804."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}], "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "3.5.2", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}, "anaconda-cloud": {}}}